\documentclass{article}

\usepackage[toc,page]{appendix}
\usepackage{float}					% floats
\usepackage[T1]{fontenc}			% Icelandic chars
\usepackage[margin=3cm]{geometry}	% margin
\usepackage{graphicx}				% figures
\usepackage[utf8]{inputenc}			% Icelandic chars
\usepackage{listings} 				% display code
\usepackage{natbib}					% references/bibliography
\usepackage[section]{placeins}		% subsections
\usepackage{xspace}					% for \Rebeca macro

\setlength{\parskip}{1em}

\bibliographystyle{plainnat}

\begin{document}

% document-specific command declarations
\newcommand{\Rebeca}{$\mathcal{R}$ebeca\xspace}

\lstdefinestyle{rebeca}{
	language=Java,
	breaklines=true,
	tabsize=2,
}
\lstdefinestyle{traces}{
	breaklines=true,
	tabsize=2,
}

\title{Heuristics for generating abstract test cases from \Rebeca model}
\author{Þröstur Thorarensen (throstur11@ru.is)}
\date{\today}
\maketitle

\begin{abstract}
	Software testing can be a difficult task for humans and is a prime candidate for automation. Concurrent systems are no exception, even when employing paradigms such as the Actor model. Model checking goes a long way to verifying software correctness, but models always abstract away from the implementation. Mutant testing is a promising method to pinpoint programming errors, by combining the abstract model of a system with mutants of the implementation, it is possible to automatically generate an extensive suite of test cases with ease. The research demonstrates that software testing can easily be supplemented by automation and that even the simplistic methods employed in this study are capable of producing reliable results.
\end{abstract}

\newpage
\tableofcontents
\newpage

	\section{Introduction}
		A model describing a System Under Test (SUT) is usually an abstract, partial presentation of the SUT's desired behavior. Model-based testing is using such a model of the SUT to generate abstract test cases and then mapping those abstract test cases to executable test cases based on the back-end code. Test cases derived from such a model are at the same level of abstraction as the model. An abstract test case cannot be directly executed against a SUT and an executable test suite needs to be derived from a corresponding abstract test suite.

		Model checking is a method of formally verifying finite-state concurrent systems according to a given specification. The specification is generally expressed as temporal logic formulas which are verified either with explicit-state model checking or symbolic model checking. Symbolic model checking is more efficient states can be represented as sets of states rather than as single states.

		\Rebeca is an actor-based modeling language designed in an effort to bridge the gap between formal verification approaches and real applications. \Rebeca introduces the ability to analyze a group of reactive objects as single components of a system in the actor model.

		Since we cannot test software with all inputs, \textbf{coverage criteria} are used to decide which test inputs to use. The software testing community believes that effective use of coverage criteria makes it more likely that test engineers will find faults in a program and provides informal assurance that the software is of high quality and reliability. 

		Mutation testing is used to design new software tests and evaluate the quality of existing software tests. It involves creating \textit{mutants} modified versions of the SUT that are based on \textit{mutation operators}. Each mutant $m \in M$ where $M$ is the set of mutants for a given artifact will lead to a test requirement\footnote{Criterion 5.32 on Mutation Coverage in \citet{ammann2008introduction}} and yields more test requirements than any other test criterion in most situations (provided that the mutation operators are well-designed). In practise, if software contains a fault, there will usually be a set of mutants that can only be killed by a test case that also detects that fault \citep{ammann2008introduction}.

	\section{Contribution}
		Coverage criteria is necessary when testing all but the most trivial software. Coverage critera can however be difficult to decide upon for complicated systems. Software developers and testers are often faced with the task of testing software. This can be error-prone and time consuming, making testing a useful target for automation.

		In this study, we modeled a case study using \Rebeca and employed model checking to ascertain that the model is faithful to the specification. Following this, we generated the state space of the model and extracted (random) traces through the graph. Although employing some heuristics for which traces are extracted would have been nice, this has been left as future work. The extracted traces were transformed into test cases for the SUT.

		We used muJava to generate mutants for the SUT. Each mutant was evaluated by the test cases in an attempt to find mutants that could not be killed. These mutants act as the coverage criteria for the randomly generated traces.

	\section{Related Work}
	\citet{DBLP:conf/birthday/SirjaniJ11} discuss \Rebeca after 10 years of experience in analyzing actors. The paper introduces \Rebeca in detail along with explaining the supporting model checking tools. The paper focuses on state-space reduction techniques which may come in handy for future work, along with \citeauthor{tasharofi2014efficient}'s dissertation on efficient testing of non-deterministic actor programs which introduces novel solutions in reducing the number of explored interleavings of a non-deterministic concurrent actor system \citep{tasharofi2014efficient}.

		\citet{ammann2008introduction} go into great detail in explaining the basics of software testing, covering concepts used in this research such as coverage criteria and mutants. Furthermore, \citet{mujava} have developed an excellent tool for Java for generating mutants, which lead to test requirements for syntax-based coverage criteria. The tool, muJava, is used in this research to generate method-level mutants.

		
	\section{Method}
		\subsection{Modeling the System Under Test}
			The case study intended as the SUT is the NASA GMSEC Message Bus, the specification of which was provided by \citeauthor{fraunhofer}. The model of the system was implemented as a \texttt{reactiveclass} in \Rebeca. Simply modeling the SUT as a \texttt{reactiveclass} was not sufficient for our purposes \-- the system is reactive and as such requires inputs from external entities to avoid idling. For this reason, a \texttt{reactiveclass} modeling a user of the SUT was implemented, non-deterministically sending messages to the SUT. To avoid a combinatorial explosion in generating the state space, the amount of messages sent must have a reasonably low upper bound \-- 20 was selected for this study.

			It should be noted that an actual implementation of the SUT was not used in this study. Instead, the model was re-implemented as a Java class following the specification of the system\footnote{Using this approach proved to be quite useful. We were able to pinpoint an error in the model due to incorrect handling of an edge case which was correct in the implementation (which our properties for model checking did not cover). Although a complete implementation should also catch this, using the complete implementation increases the complexity of the project and reduces the amount of time that could be dedicated solely to the research topic at hand. }. This was done for a few reasons, notably giving control over used frameworks to the researchers and to double-check that the model is faithful to the specification. The SUT implementation was implemented as an Akka Java actor since the Akka Testkit framework provides excellent tools to test actor based systems.

		\subsection{Coverage Criteria}
			\label{sec:coveragecrit}
			The coverage criteria for the test suite generated in this research is the set of all mutants that must be killed. Mutation testing is a kind of syntax-based testing, where the coverage criteria is the syntactic description of a software artifact. Recall that \textit{Criterion 5.32} in \citet{ammann2008introduction} states that each mutant $m \in M$ where $M$ is the set of mutants for a given artifact will lead to a test requirement \-- each test requirement produced by the elements in $M$ is a part of our coverage criteria.
			The test suite consists of a tuple $(M, T)$, where $M$ is the set of mutants and $T$ is a list of traces\footnote{It would be more useful to have a \textit{set} of traces, but since randomness does not guarantee uniqueness, we will make do with a list.}. Only messages to and from the SUT are considered (all other transitions in the state space are considered as $\tau$ transitions and were discarded from the test case generation). The state space of the model was exported from Afra (\Rebeca IDE) as an \texttt{.aut}
			\footnote{ALDEBARAN/\textit{AUTomaton} file format. Specification: \texttt{http://www.inrialpes.fr/vasy/cadp/man/aut.html\#sect2}} file suitable for visualization with software such as CADP.

		\subsection{Generating Test Cases}
			\label{sec:method_testgen}
			\subsubsection{Abstract Test Cases}
				\label{sec:method_abstract}
				Random traces were extracted from the state space. The traces act as abstract test cases for the SUT, as the behavior of an actor based model to an outside observer can be fully modeled based only on the messages that are being sent between actors.

				In order to extract individual traces from the \Rebeca model, we implemented simple Java software to create a graph of the state space based on an \texttt{.aut} file as input, with the output being a list of traces through the graph. Branching decisions in the state space were made at random. The traces extracted from the state space are then used as abstract test cases.
				The code can be found in the \texttt{trace-extract} package of the \texttt{trace-mutants} GitHub repository. \nocite{trace-mutants}

			\subsubsection{Concrete Test Cases}
				\label{sec:method_concrete}
				The \texttt{trace-extract} package also includes a Java class for transforming the abstract test cases to concrete test cases. The \texttt{TestGenerator} class generates a Scala test spec for the Akka Testkit framework\footnote{The \texttt{TestGenerator} class hard-codes the messages that should be sent and received by the SUT as a result of fast prototyping, but the code is available on GitHub}. The executable test cases are quite simple in nature \-- messages are sent to the SUT and the test environment expects a \texttt{Success} or a \texttt{Failure} message in return. If the test environment receives an unexpected message, the test case is considered as failed\footnote{As discussed in later sections, the goal is to get test cases to fail (hopefully sooner rather than later).}.

		\subsection{Mutant Generation}
			\label{sec:method_mutgen}
			Mutants were generated using the muJava mutation system for Java programs \citep{mujava}. Some slight modifications were made to the source code of muJava to facilitate mutant compilation.The artifacts of the modifications are available in the \texttt{gen-mutants} package of the \texttt{trace-mutants} GitHub repository. Every available mutant operator was applied to the original source, generating a total of 316 mutants.

			Class mutants can be generated by muJava by using class-level mutation operators. This was not done since the simplified implementation consisted mostly of a single class (with inner classes as message types). muJava defines 12 method-level operators which were all applied to the SUT implementation. Arithmetic, conditional and logical operators can be either \textit{replaced}, \textit{deleted}, or \textit{inserted}\footnote{Only unary operators can be inserted.} in the code. Relational, shift and assignment operators can be replaced but insertion or deletion is not done. \citet{mutopsMethod} provide further details for mutation operators. Mutants that do not compile are considered killed, but are not counted towards to total number of mutants.

		\subsection{Test Case Evaluation}
			\label{sec:method_testing}
			The test cases in the generated test suite can be evaluated with the \texttt{evaluate-mutants} package of the \texttt{trace-mutants} GitHub repository. \texttt{evaluate-mutants} uses the file system to set up individual environments for each mutant to be compiled and tested in and then uses \citeauthor{sbt} to execute the test cases.

			The test suite attempts to kill each mutant by running the executable test cases. If a mutant fails some test case, it is considered killed. The goal of a test suite is to kill all mutants, although determining which mutants are not being killed can be beneficial, especially for further development of the coverage criteria.

			The result of a test case evaluation can give different information depending on the results. \\
			If all mutants are killed, either:
				\begin{itemize}
					\item The coverage of the test suite is insufficient (generally due to lacking mutation operators); or
					\item The SUT conforms to the model and the test suite satisfies the coverage criteria.
				\end{itemize}
			Otherwise:
				\begin{itemize}
					\item There is a bug in the SUT, representative of a mutant that was not killed; or
					\item There is a discrepancy between the SUT and the model.
				\end{itemize}

	\section{Results}
		Although muJava attempted to generate 356 mutants, only 316 mutants of our implementation of the SUT could be compiled. 277 of those mutants modified the \texttt{onReceive(Object message)} method whereas 79 modified other parts of the program, including the message constructors and the overloaded \texttt{boolean equals(Object other)} methods.

		We found that 50 random traces from the state space did not generate a sufficiently diverse set of traces to reliably kill all mutants.
		In one run, 5 mutants survived the 50 randomly chosen traces\footnote{data/0050\_5\_0.traces}: \texttt{COR\_21, ODL\_33, SDL\_17, SDL\_19, VDL\_14}. The \texttt{SDL} mutants modified the \texttt{boolean equals(Object other)} method whereas the other mutants modified the \texttt{onReceive(Object message)} method. \texttt{ODL\_33} and \texttt{VDL\_13} resulted in the same mutation (removing a predicate from an \texttt{OR} conditional) whereas \texttt{COR\_21} replaced the \texttt{OR} operator to an \texttt{AND} operator.

		Similar results were encountered during development of the test framework. Since randomness plays such a large part in determining whether or not a set of traces through the state space graph provides adequate coverage. We found that generating 1000 traces resulted in all mutants being killed every time, although theoretically it is of course always possible to generate unhelpful test cases when using randomness. We also tested 250 traces\footnote{data/0250\_0\_0.traces} with all mutants being killed, while 100 traces\footnote{data/0100\_4\_0.traces} did not prove to be sufficient (4 survivors, the same as for the 50-trace run with the exception of \texttt{COR\_21}). Due to the nature of randomness, we cannot guarantee that any number of traces will invariantly kill all mutants (provided that killing all mutants is at all possible) \-- at least not until some heuristics are applied to trace extraction.

		We were able to find one discrepancy between the model and the implementation, in which an edge case was being handled incorrectly in the model but correctly in the implementation. This demonstrates that in order to kill all mutants, it is important that the model and implementation be faithful to each other. Mutants generated from a correct implementation but an incorrect model are more difficult to kill and create gaps in the coverage, as do mutants generated from an incorrect implementation but a correct model.

		We encountered some issues with the chosen build tool, \citeauthor{sbt}. It is not well suited for testing on parallel distributed file systems as it requires file system locking, a feature that is not generally present on file systems residing on networked storage. We therefore replaced the tool with a simple fail-fast build script, which has the potential to increase performance. The performance increase depends on randomness\footnote{When abstract test cases are compiled into concrete test cases, it's possible to split them into batch sizes. This introduces slight overhead for each generated batch but if a mutant fails on the first batch, then the rest will be ignored.}.


	\section{Conclusion}
		The research shows that by modeling a SUT, it is possible to easily create many abstract test cases from the state space of the model. The research does not go into heuristics for selecting the best abstract test cases but uses randomness instead. We found that 100 abstract test cases extracted from the state space of the model did not reliably satisfy the coverage criteria, but larger amounts such as 250 and 1000 managed to satisfy the criteria.

		The goal of using an automatically generated test-suite such as the one demonstrated in this study is to kill all mutants. If all mutants can be killed, then either the SUT conforms to the specifications applied to the model or more test requirements are necessary to detect bugs. A failing test suite will uncover a fault in either the model or implementation.

		The results are indicative that certain mutants are less likely to be killed as the number of abstract test cases decreases when compared with other mutants. It might be beneficial to pinpoint these mutants and use them as inexpensive coverage criteria for regression testing. Those mutants could also possibly reveal sensitive areas in the system.

	\section{Future Work}
		Despite the contibution to the field made by this study, there is still much research left to be explored. Most notably, the work should be repeated with a complete implementation of a system rather than with an implementation that is deliberately nearly identical to the model itself. Increased complexity in a system's implementation can introduce hidden side effects that could possibly affect the results of the test cases.

		Furthermore, due to the unexpected issues with \citeauthor{sbt} on parallel distributed file systems, the build system (which acts also as the test runner) should be replaced by a more suitable alternative. This has been done in part, but remains to be tested on distributed systems\footnote{The build system was replaced with a bash script in an effort to allow tests to be run on a high-performance computing cluster, but the cluster was ongoing maintenance and the required tools were out-of-date, if at all present.}.

		A highly desireable improvement on the current work would be to add some sort of heuristics to the selection of traces from the state space of the system. Currently the traces are chosen at random with no guarantees of any actual coverage \-- with actual heuristics in place it will be possible to guarantee some sort of coverage rather than depending on randomness.

		There seems to be a bias towards specific mutants not being killed even when completely different random traces are used as abstract test cases. It might be useful to explore where this bias comes from and what other information could be hiding in those mutants.

\newpage
\bibliography{references}

\newpage
\begin{appendices}

\section{\Rebeca Model - Full Code}

\lstinputlisting[style=rebeca]{ext/bus1.rebeca}

The \Rebeca model includes two \texttt{rebec}s: the message bus itself and an application (\texttt{Attacker}) that sends messages to the message bus at random. This ensures that as long as the \texttt{MessageBus rebec} is correct, the generated state space will represent the state space of a correct implementation of the SUT.

\newpage
\section{Implementation - Full Code}

\lstinputlisting[style=rebeca]{ext/JBus.java}

The implementation is analogous to the \Rebeca model. Details are omitted but the the implementation and model are faithful to each other. 

%\newpage
%\section{Data Files}
%
%\lstinputlisting[style=traces,caption=data/0050\_5\_0.traces]{data/0050_5_0.traces}
%
%\lstinputlisting[style=traces,caption=data/0250\_0\_0.traces]{data/0250_0_0.traces}
%

\end{appendices}

\end{document}
